# GPT124M
This repository contains my implementation of the GPT-2 architecture from scratch, inspired by Andrej Karpathy’s Neural Networks: Zero to Hero series and OpenAI’s original GPT-2 model. The goal is to deeply understand the inner workings of transformer-based language models by building each component from the ground up.
